{"cells":[{"cell_type":"markdown","metadata":{"id":"MdPRp4q7iisZ"},"source":["# BERT Tone Indicators Classification Model \n","\n","\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":21466,"status":"ok","timestamp":1651511112488,"user":{"displayName":"Eigen Ott","userId":"15016988378050056333"},"user_tz":240},"id":"G552kmZMHZth","outputId":"fec9b894-f486-4052-bd7d-fe9d7eb86ece"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.18.0)\n","Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.12.1)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.6.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.5.1)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.3)\n","Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.50)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.2.0)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.8)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n","Requirement already satisfied: click==8.0 in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (8.0.0)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n","Requirement already satisfied: emoji in /usr/local/lib/python3.7/dist-packages (1.7.0)\n","Requirement already satisfied: langdetect in /usr/local/lib/python3.7/dist-packages (1.0.9)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from langdetect) (1.15.0)\n","  Building wheel for torchviz (setup.py) ... \u001b[?25l\u001b[?25hdone\n"]}],"source":["!pip install transformers --quiet\n","!pip install emoji --quiet\n","!pip install langdetect --quiet\n","!pip install torchviz --quiet"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1651511116692,"user":{"displayName":"Eigen Ott","userId":"15016988378050056333"},"user_tz":240},"id":"cKIWwCsUigIp","outputId":"eead956e-9dd8-4273-909f-5bde3a7053b2"},"outputs":[{"output_type":"stream","name":"stdout","text":["numpy 1.21.6\n","pandas 1.3.5\n"]}],"source":["import os\n","\n","import numpy as np\n","print('numpy', np.__version__)\n","import pandas as pd\n","print('pandas', pd.__version__)\n","from langdetect import detect\n","import random\n","import re\n","\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","#from torchtext.data import Field, TabularDataset, BucketIterator, Iterator\n","\n","from transformers import AutoTokenizer, BertTokenizer, BertForSequenceClassification\n","\n","from tqdm import tqdm #progress bar\n","\n","# Evaluation\n","\n","from sklearn.metrics import accuracy_score, classification_report, confusion_matrix"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1922,"status":"ok","timestamp":1651511120206,"user":{"displayName":"Eigen Ott","userId":"15016988378050056333"},"user_tz":240},"id":"dstqlyfGj7cL","outputId":"b509b85e-e7e6-4b83-9188-6e35edaacedf"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oxjZHmBgkSNH","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1651511121577,"user_tz":240,"elapsed":16,"user":{"displayName":"Eigen Ott","userId":"15016988378050056333"}},"outputId":"982ff40f-e92d-4408-efaf-a7de73495b8c"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/.shortcut-targets-by-id/16W2OVxf4pSh-3dRxed_aDRKcUuYcKpsj/Twitter\n"]}],"source":["cd /content/drive/My Drive/Twitter"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mpL5JTg4G1Gn"},"outputs":[],"source":["import time\n","import datetime\n","\n","def format_time(elapsed):\n","    '''\n","    Takes a time in seconds and returns a string hh:mm:ss\n","    '''\n","    # Round to the nearest second.\n","    elapsed_rounded = int(round((elapsed)))\n","    \n","    # Format as hh:mm:ss\n","    return str(datetime.timedelta(seconds=elapsed_rounded))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":502,"status":"ok","timestamp":1651511123578,"user":{"displayName":"Eigen Ott","userId":"15016988378050056333"},"user_tz":240},"id":"7dbO0815G72P","outputId":"cd7936ba-2be3-4bd3-beae-3bee6411ba42"},"outputs":[{"output_type":"stream","name":"stdout","text":["Device count: 1\n","Using device: cuda\n","Tesla K80\n","_CudaDeviceProperties(name='Tesla K80', major=3, minor=7, total_memory=11441MB, multi_processor_count=13)\n","Memory Usage:\n","Allocated: 0.0 GB\n","Cached:    0.0 GB\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/cuda/memory.py:386: FutureWarning: torch.cuda.memory_cached has been renamed to torch.cuda.memory_reserved\n","  FutureWarning)\n"]}],"source":["from torch import cuda\n","\n","device = 'cuda' if cuda.is_available() else 'cpu'\n","print('Device count:', cuda.device_count())\n","print('Using device:', device)\n","\n","#Additional Info when using cuda\n","if device == 'cuda':\n","    print(torch.cuda.get_device_name(0))\n","    print(torch.cuda.get_device_properties(0))\n","    print('Memory Usage:')\n","    print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n","    print('Cached:   ', round(torch.cuda.memory_cached(0)/1024**3,1), 'GB')"]},{"cell_type":"markdown","metadata":{"id":"TIAM_8w-jyVJ"},"source":["# Load tweets from file"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":472},"executionInfo":{"elapsed":1487,"status":"ok","timestamp":1651511127678,"user":{"displayName":"Eigen Ott","userId":"15016988378050056333"},"user_tz":240},"id":"KTbKkIxKj23n","outputId":"0898b0e2-de5f-44d5-80b6-085f281d1a77"},"outputs":[{"output_type":"stream","name":"stdout","text":["Index(['text', 'srs', 'nbh', 'hyp', 'pos', 'neg'], dtype='object')\n"]},{"output_type":"execute_result","data":{"text/plain":["                                                                  text  srs  \\\n","tid                                                                           \n","1491924924386230287                RT @malewifealtair: @sobokify /srs.    1   \n","1491924858493620227        i just swallowed a small gemstone /srs /pos    1   \n","1491924846242152457  @bonsorlol no bc it genuinely had great themes...    0   \n","1491924808476422144  registering for classes /neg üòµ‚Äçüí´üòµ‚Äçüí´üòµ‚Äçüí´ i am so...    0   \n","1491924691925311489  I found out about fnf a week or two after it c...    1   \n","...                                                                ...  ...   \n","1495361726807560194  always show up on my tl /pos but were not clos...    0   \n","1495361724601417729                                       screams /neg    0   \n","1495361646067392520            @walterhillerska i hate gay people /srs    1   \n","1495361617466621955                are u comfy with this user ?? /srs.    1   \n","1495361430178074626        damn he actually did kiss the ice im üò≠ /pos    0   \n","\n","                     nbh  hyp  pos  neg  \n","tid                                      \n","1491924924386230287    0    0    0    0  \n","1491924858493620227    0    0    1    0  \n","1491924846242152457    0    0    1    0  \n","1491924808476422144    0    0    0    1  \n","1491924691925311489    0    0    0    0  \n","...                  ...  ...  ...  ...  \n","1495361726807560194    0    0    1    0  \n","1495361724601417729    0    0    0    1  \n","1495361646067392520    0    0    0    0  \n","1495361617466621955    0    0    0    0  \n","1495361430178074626    0    0    1    0  \n","\n","[149847 rows x 6 columns]"],"text/html":["\n","  <div id=\"df-340eed42-8e79-4f2a-8a1d-428fc7c08ba4\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>text</th>\n","      <th>srs</th>\n","      <th>nbh</th>\n","      <th>hyp</th>\n","      <th>pos</th>\n","      <th>neg</th>\n","    </tr>\n","    <tr>\n","      <th>tid</th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>1491924924386230287</th>\n","      <td>RT @malewifealtair: @sobokify /srs.</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1491924858493620227</th>\n","      <td>i just swallowed a small gemstone /srs /pos</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1491924846242152457</th>\n","      <td>@bonsorlol no bc it genuinely had great themes...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1491924808476422144</th>\n","      <td>registering for classes /neg üòµ‚Äçüí´üòµ‚Äçüí´üòµ‚Äçüí´ i am so...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1491924691925311489</th>\n","      <td>I found out about fnf a week or two after it c...</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>1495361726807560194</th>\n","      <td>always show up on my tl /pos but were not clos...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1495361724601417729</th>\n","      <td>screams /neg</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1495361646067392520</th>\n","      <td>@walterhillerska i hate gay people /srs</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1495361617466621955</th>\n","      <td>are u comfy with this user ?? /srs.</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1495361430178074626</th>\n","      <td>damn he actually did kiss the ice im üò≠ /pos</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>149847 rows √ó 6 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-340eed42-8e79-4f2a-8a1d-428fc7c08ba4')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-340eed42-8e79-4f2a-8a1d-428fc7c08ba4 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-340eed42-8e79-4f2a-8a1d-428fc7c08ba4');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":26}],"source":["data = pd.read_csv('all_tweets_english.tsv', delimiter='\\t', index_col=0, on_bad_lines='skip')\n","print(data.columns)\n","data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wNyLfih3mdQf"},"outputs":[],"source":["# sum_data = data['srs']+data['nbh']+data['hyp']+data['pos']+data['neg']\n","# count = 0\n","# for i, c in enumerate(sum_data):\n","#   if c >1:\n","#     print(i, c, data['text'][i])\n","#     count +=1\n","# print(count)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1461,"status":"ok","timestamp":1651511131295,"user":{"displayName":"Eigen Ott","userId":"15016988378050056333"},"user_tz":240},"id":"yNaoV7zTpT-v","outputId":"7e170a80-9ecc-4275-a77e-600dcfdf9a42"},"outputs":[{"output_type":"stream","name":"stdout","text":["The rate of tweets with more than 1 tags: 0.01030066815144766\n"]}],"source":["#rate of more than 1 label\n","print('The rate of tweets with more than 1 tags:', 111/10776)"]},{"cell_type":"markdown","metadata":{"id":"EGdHt76VG-yf"},"source":["# Data Preprocessing\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1348,"status":"ok","timestamp":1651511135181,"user":{"displayName":"Eigen Ott","userId":"15016988378050056333"},"user_tz":240},"id":"rDrSrgh9W3Rv","outputId":"6f8e5439-4402-46b1-b0d7-126d312caa5a"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:2: FutureWarning: The default value of regex will change from True to False in a future version.\n","  \n"]},{"output_type":"stream","name":"stdout","text":["                                                                  text  srs  \\\n","tid                                                                           \n","1491924924386230287                RT @malewifealtair: @sobokify /srs.    1   \n","1491924858493620227        i just swallowed a small gemstone /srs /pos    1   \n","1491924846242152457  @bonsorlol no bc it genuinely had great themes...    0   \n","1491924808476422144  registering for classes /neg üòµ‚Äçüí´üòµ‚Äçüí´üòµ‚Äçüí´ i am so...    0   \n","1491924691925311489  I found out about fnf a week or two after it c...    1   \n","...                                                                ...  ...   \n","1495361726807560194  always show up on my tl /pos but were not clos...    0   \n","1495361724601417729                                       screams /neg    0   \n","1495361646067392520            @walterhillerska i hate gay people /srs    1   \n","1495361617466621955                are u comfy with this user ?? /srs.    1   \n","1495361430178074626        damn he actually did kiss the ice im üò≠ /pos    0   \n","\n","                     nbh  hyp  pos  neg  \n","tid                                      \n","1491924924386230287    0    0    0    0  \n","1491924858493620227    0    0    1    0  \n","1491924846242152457    0    0    1    0  \n","1491924808476422144    0    0    0    1  \n","1491924691925311489    0    0    0    0  \n","...                  ...  ...  ...  ...  \n","1495361726807560194    0    0    1    0  \n","1495361724601417729    0    0    0    1  \n","1495361646067392520    0    0    0    0  \n","1495361617466621955    0    0    0    0  \n","1495361430178074626    0    0    1    0  \n","\n","[149829 rows x 6 columns]\n"]}],"source":["contain_values = data[data['text'].str.contains('/srs|/nbh|/hyp|/pos|/neg')]\n","contain_values[\"text\"] = contain_values[\"text\"].str.replace(r'\\s*https?://\\S+(\\s+|$)', ' ').str.strip()\n","pd.options.mode.chained_assignment = None  # default='warn'\n","labeled = contain_values\n","labeled[\"srs\"] = np.where(labeled[\"text\"].str.contains('/srs'), 1, 0)\n","labeled[\"nbh\"] = np.where(labeled[\"text\"].str.contains('/nbh'), 1, 0)\n","labeled[\"hyp\"] = np.where(labeled[\"text\"].str.contains('/hyp'), 1, 0)\n","labeled[\"pos\"] = np.where(labeled[\"text\"].str.contains('/pos'), 1, 0)\n","labeled[\"neg\"] = np.where(labeled[\"text\"].str.contains('/neg'), 1, 0)\n","data = labeled\n","print(data)"]},{"cell_type":"markdown","source":["Uncomment and run the next cell if your dataset is not filtered for only english tweets containing tone indictors. Otherwise if your data is clean, skip it "],"metadata":{"id":"Mjft-Hn_oKcL"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"elapsed":543,"status":"ok","timestamp":1651511137186,"user":{"displayName":"Eigen Ott","userId":"15016988378050056333"},"user_tz":240},"id":"y_4pNUv4hwgX","outputId":"09b01f2e-d87a-45e7-e30c-4ff44556bf5d"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["\"\\ndf_new = data[data.text.apply(detect2).eq('en')]\\nprint(df_new)\\ndata = df_new\\n\""],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":30}],"source":["# The fastest runtime comes from applying this function to the entire df\n","\"\"\"\n","def detect2(row):\n","  res = \"error\"\n","  try:\n","    res = detect(row)\n","    return res\n","  except:\n","    return res\n","\"\"\"\n","\n","# Example Run:\n","'''\n","df = pd.read_csv(\"in.tsv.tsv\",sep='\\t')\n","contain_values = df[df['text'].str.contains('/srs|/nbh|/hyp|/pos|/neg')]\n","contain_values[\"text\"] = contain_values[\"text\"].str.replace(r'\\s*https?://\\S+(\\s+|$)', ' ').str.strip()\n","df_new = contain_values[contain_values.text.apply(detect2).eq('en')]\n","print(df_new)\n","df_new.to_csv('out.tsv', sep=\"\\t\")\n","'''\n","\n","#First we exclude rows that don't have the tags, then exclude rows that have URLs\n","#Use detect2 for the final frame and convert to tsv\n","\n","'''\n","df_new = data[data.text.apply(detect2).eq('en')]\n","print(df_new)\n","data = df_new\n","'''"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nJA7_MeZzY5u","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1651511137591,"user_tz":240,"elapsed":6,"user":{"displayName":"Eigen Ott","userId":"15016988378050056333"}},"outputId":"53b10dac-64c1-43e5-90e5-458f104fef43"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'\\nprint(df_new)\\ndf_new.to_csv(\\'all_tweets_english.tsv\\', sep=\"\\t\")\\n'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":31}],"source":["# sava cleaned data \n","'''\n","print(df_new)\n","df_new.to_csv('all_tweets_english.tsv', sep=\"\\t\")\n","'''\n"]},{"cell_type":"markdown","metadata":{"id":"yRLrd4boHoDT"},"source":["## Tokenization"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1651511139078,"user":{"displayName":"Eigen Ott","userId":"15016988378050056333"},"user_tz":240},"id":"5lPPRZDZHyvE","outputId":"e69674ac-4c6b-46cc-ecb9-3eb18f618ba2"},"outputs":[{"output_type":"stream","name":"stdout","text":["<class 'pandas.core.frame.DataFrame'>\n","RangeIndex: 147807 entries, 0 to 147806\n","Data columns (total 6 columns):\n"," #   Column  Non-Null Count   Dtype \n","---  ------  --------------   ----- \n"," 0   text    147807 non-null  object\n"," 1   srs     147807 non-null  int64 \n"," 2   nbh     147807 non-null  int64 \n"," 3   hyp     147807 non-null  int64 \n"," 4   pos     147807 non-null  int64 \n"," 5   neg     147807 non-null  int64 \n","dtypes: int64(5), object(1)\n","memory usage: 6.8+ MB\n","None\n","srs    51329\n","nbh     4080\n","hyp      105\n","pos    78147\n","neg    14146\n","dtype: int64\n"]}],"source":["#remove the tweets with more than 1 tag\n","tones = ['srs', 'nbh', 'hyp', 'pos', 'neg']\n","\n","filter = data[tones].sum(axis=1)==1\n","data = data[filter].reset_index(drop=True)\n","print(data.info())\n","print(data[tones].sum(axis=0))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":33779,"status":"ok","timestamp":1651511173955,"user":{"displayName":"Eigen Ott","userId":"15016988378050056333"},"user_tz":240},"id":"PpHgC4yhUXls","outputId":"577aaab1-55e5-4814-bfc3-e572e38b12cf"},"outputs":[{"output_type":"stream","name":"stdout","text":["                                                     text  srs  nbh  hyp  pos  \\\n","0                     RT @malewifealtair: @sobokify /srs.    1    0    0    0   \n","1       @bonsorlol no bc it genuinely had great themes...    0    0    0    1   \n","2       registering for classes /neg üòµ‚Äçüí´üòµ‚Äçüí´üòµ‚Äçüí´ i am so...    0    0    0    0   \n","3       I found out about fnf a week or two after it c...    1    0    0    0   \n","4                                   @malewifealtair /srs?    1    0    0    0   \n","...                                                   ...  ...  ...  ...  ...   \n","147802  always show up on my tl /pos but were not clos...    0    0    0    1   \n","147803                                       screams /neg    0    0    0    0   \n","147804            @walterhillerska i hate gay people /srs    1    0    0    0   \n","147805                are u comfy with this user ?? /srs.    1    0    0    0   \n","147806        damn he actually did kiss the ice im üò≠ /pos    0    0    0    1   \n","\n","        neg  tag  \n","0         0  srs  \n","1         0  pos  \n","2         1  neg  \n","3         0  srs  \n","4         0  srs  \n","...     ...  ...  \n","147802    0  pos  \n","147803    1  neg  \n","147804    0  srs  \n","147805    0  srs  \n","147806    0  pos  \n","\n","[147807 rows x 7 columns]\n"]}],"source":["# this can take 1-2 minutes to run\n","data['tag']=data[tones].apply(lambda row: row[row==1].index[0], axis=1)\n","\n","label2ind = {tag: tones.index(tag) for tag in tones}\n","tags_token= data['tag'].apply(lambda x: label2ind[x])\n","\n","print(data)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SHVO7EHvUpQz"},"outputs":[],"source":["# replace all the tags in texts with [unknown token]\n","data['text'] = data['text'].str.replace('/srs','TONE_INDICATOR')\n","data['text'] = data['text'].str.replace('/nbh','TONE_INDICATOR')\n","data['text'] = data['text'].str.replace('/pos','TONE_INDICATOR')\n","data['text'] = data['text'].str.replace('/hyp','TONE_INDICATOR')\n","data['text'] = data['text'].str.replace('/neg','TONE_INDICATOR')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":254,"referenced_widgets":["75327c0805c540139344cd13775dad5f","54899fe4c6cc4d99be2a984eca860ada","62569192ccaa44fcaf5be39af207d215","f4515028018e41c38f63ce0b3eb68a94","fb694c220095436c907503c6c85a2d8a","7232f5f528c84b16bb78d4e352817dcd","b8834817f001469f9c8935e7a057e74d","96a46f254b554a1bb44aa3edc7cd4d9f","8931cd75a3064a3e9c945aac5c046982","e48e2a66020a4503a2463c69abfaa832","e4e6da985d76452f8592ac2777fddca3","35d8c1fe62264e288919a608b663c2a2","4548174d0a454bddbd66c67db2f29782","63e8fe9e66eb4cf4a7464b1e775500a2","fedb4684404f4d53b1114455c9f37841","a3269b135b4e448487d41b0f71214d8e","5dd00853ffb04eaea21a7f69df5fcf4a","b07421e0e7e643d6a1669b70d4661192","530ba5b67f9b4fdd890872b99aacacbe","cbc4d1238878448ea9406625863c7f07","fb65c6f50d584287ac6bf8905b2feeb6","7ffdfff774744073b967a7a7afbdcfbc","ff5cca2f47f54621a1523329e416c350","30f9606a5247433e8cf29b439aaaf31b","3c14376fe3ad460c9acd9ade6f75219e","b7530abbae114d00ba6da5f44f5430c8","7fe41d5539fc4e2d8de551de57734396","e04a06f09e6e47f38800e9a2d1bff60b","a66d8485ce4e4bf6b01260fc1d7bf9e7","75dc32e3021f4f7e892b35ef613dcad0","1ed5352d7d124896b041bf00a6cb6604","f280f4bb66d141f09dbf39c01fb3f96c","5d0f7b32128749c7b4dee243be0400ff"]},"executionInfo":{"elapsed":5546,"status":"ok","timestamp":1651511030987,"user":{"displayName":"Eigen Ott","userId":"15016988378050056333"},"user_tz":240},"id":"-cPlTjd8VU0Z","outputId":"e1494483-5546-4d07-88d4-eea04bc68c76"},"outputs":[{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/558 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"75327c0805c540139344cd13775dad5f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/824k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"35d8c1fe62264e288919a608b663c2a2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/1.03M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ff5cca2f47f54621a1523329e416c350"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"]},{"output_type":"stream","name":"stdout","text":["length before new token 64001\n","length after new token 64001\n","im going to cry we didnt look at eachother or even fucking look at eachother :// TONE_INDICATOR\n"," Original:  im going to cry we didnt look at eachother or even fucking look at eachother :// TONE_INDICATOR\n","Tokenized:  ['im', 'going', 'to', 'cry', 'we', 'didnt', 'look', 'at', 'eachother', 'or', 'even', 'fucking', 'look', 'at', 'eachother', ':@@', '/@@', '/', 'TON@@', 'E_@@', 'IND@@', 'IC@@', 'ATOR']\n","Token IDs:  [199, 117, 9, 866, 54, 1403, 184, 35, 9317, 72, 132, 309, 184, 35, 9317, 1043, 2603, 75, 16457, 35637, 16065, 2965, 22679]\n"]}],"source":["#tokenize sentences\n","tokenizer = AutoTokenizer.from_pretrained(\"vinai/bertweet-base\", use_fast=False)\n","print(\"length before new token\", len(tokenizer))\n","#tokenizer.add_tokens([\"TONE_INDICATOR\"])\n","print(\"length after new token\", len(tokenizer))\n","\n","tweets = data['text']\n","print(tweets[23])\n","print(' Original: ', tweets[23])\n","print('Tokenized: ', tokenizer.tokenize(tweets[23]))\n","print('Token IDs: ', tokenizer.convert_tokens_to_ids(tokenizer.tokenize(tweets[23])))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":51087,"status":"ok","timestamp":1651511082071,"user":{"displayName":"Eigen Ott","userId":"15016988378050056333"},"user_tz":240},"id":"1PvwjvyDdOmj","outputId":"60704abf-1de8-432d-ae32-0540b73cbbd6"},"outputs":[{"output_type":"stream","name":"stderr","text":["Token indices sequence length is longer than the specified maximum sequence length for this model (165 > 128). Running this sequence through the model will result in indexing errors\n"]},{"output_type":"stream","name":"stdout","text":["Max sentence length:  284\n"]}],"source":["#get max length of sentence token\n","max_len = 0\n","count = 0\n","\n","for twt in tweets:\n","    input_ids = tokenizer.encode(twt, add_special_tokens=True) #add `[CLS]` and `[SEP]` tokens.\n","    max_len = max(max_len, len(input_ids))\n","\n","print('Max sentence length: ', max_len)\n","# tokenizer.max_len_sentences_pair, tokenizer.max_len_single_sentence"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":52127,"status":"ok","timestamp":1651511252569,"user":{"displayName":"Eigen Ott","userId":"15016988378050056333"},"user_tz":240},"id":"Nk9sqfxDddKV","outputId":"063d8a54-97b0-418c-8d13-3e9c5c093cfe"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2269: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"]},{"output_type":"stream","name":"stdout","text":["Original:  RT @malewifealtair: @sobokify TONE_INDICATOR.\n","Token IDs: tensor([    0,   246,  5238,  4151,   515,  2583, 15590,   836,  1257,  1784,\n","           22,  5238,  2266,  1384,   409, 16044, 16457, 35637, 16065,  2965,\n","         3368,  3447,     4,     2,     1,     1,     1,     1,     1,     1,\n","            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n","            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n","            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n","            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n","            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n","            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n","            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n","            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n","            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n","            1,     1,     1,     1])\n"]}],"source":["# encode tokenized sentences \n","\n","input_ids = []\n","attention_masks = []\n","\n","for twt in tweets:\n","    encoded_dict = tokenizer.encode_plus(\n","                        twt,                      # Sentence to encode.\n","                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n","                        max_length = 124,           # Pad & truncate all sentences.\n","                        pad_to_max_length = True,\n","                        return_attention_mask = True,   # Construct attn. masks.\n","                        return_tensors = 'pt',     # Return pytorch tensors.\n","                   )\n","        \n","    input_ids.append(encoded_dict['input_ids'])\n","    attention_masks.append(encoded_dict['attention_mask'])\n","\n","\n","input_ids = torch.cat(input_ids, dim=0)\n","attention_masks = torch.cat(attention_masks, dim=0)\n","labels = torch.tensor(tags_token)\n","\n","# Print sentence 0, now as a list of IDs.\n","print('Original: ', tweets[0])\n","print('Token IDs:', input_ids[0])"]},{"cell_type":"markdown","metadata":{"id":"0gHTVvo1gHxU"},"source":["# Train/test split, dataloader"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":330,"status":"ok","timestamp":1651511273871,"user":{"displayName":"Eigen Ott","userId":"15016988378050056333"},"user_tz":240},"id":"YCxaxtLRgG7_","outputId":"447f113f-0660-4643-b863-532a6040ed52"},"outputs":[{"output_type":"stream","name":"stdout","text":["133,026 training samples\n","14,781 validation samples\n"]}],"source":["from torch.utils.data import TensorDataset, random_split\n","\n","dataset = TensorDataset(input_ids, attention_masks, labels)\n","\n","# Create a 90-10 train-validation split.\n","train_size = int(0.9 * len(dataset))\n","val_size = len(dataset) - train_size\n","train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n","\n","print('{:>5,} training samples'.format(train_size))\n","print('{:>5,} validation samples'.format(val_size))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Y3i3g145oMQ2"},"outputs":[],"source":["from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n","\n","# The DataLoader needs to know our batch size for training, so we specify it \n","# here. For fine-tuning BERT on a specific task, the authors recommend a batch \n","# size of 16 or 32.\n","batch_size = 32\n","\n","# Create the DataLoaders for our training and validation sets.\n","# We'll take training samples in random order. \n","train_dataloader = DataLoader(\n","            train_dataset,  # The training samples.\n","            sampler = RandomSampler(train_dataset), # Select batches randomly\n","            batch_size = batch_size # Trains with this batch size.\n","        )\n","\n","# For validation the order doesn't matter, so we'll just read them sequentially.\n","validation_dataloader = DataLoader(\n","            val_dataset, # The validation samples.\n","            sampler = SequentialSampler(val_dataset), # Pull out batches sequentially.\n","            batch_size = batch_size # Evaluate with this batch size.\n","        )"]},{"cell_type":"markdown","metadata":{"id":"JRMi6UOkHJ_I"},"source":["# Model\n","model architecture:\n","pretrained-BERT + classifier layer"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZoUBracGpnr2"},"outputs":[],"source":["class BERT_Arch(nn.Module):\n","    def __init__(self, bert, output_size):\n","        super(BERT_Arch, self).__init__()\n","\n","        self.bert = bert\n","        \n","        self.fc1 = nn.Linear(768,512)\n","        self.fc2 = nn.Linear(512, 128)\n","        self.fc3 = nn.Linear(128, output_size)\n","        \n","        #activation functions\n","        self.dropout1 = nn.Dropout(0.1)\n","        self.dropout2 = nn.Dropout(0.1)\n","        self.relu1 =  nn.ReLU()\n","        self.relu2 =  nn.ReLU()\n","        self.softmax = nn.LogSoftmax(dim=1)\n","        \n","    def forward(self, sent_id, mask):\n","        \n","        bert_out = self.bert(sent_id, attention_mask=mask).pooler_output #(batch_size, hidden_size)\n","      \n","        x = self.fc1(bert_out)\n","        x = self.relu1(x)                   \n","        x = self.dropout1(x)\n","\n","        x = self.fc2(x)\n","        x = self.relu2(x)             \n","        x = self.dropout2(x)\n","                     \n","        x = self.fc3(x)\n","        x = self.softmax(x)\n","                     \n","        return x\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pRMrAD7xVUu1"},"outputs":[],"source":["!CUDA_LAUNCH_BLOCKING=1"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":138,"referenced_widgets":["7da46a6425314ae5a83531b1949154b7","35a7a8866ac94c4a80c69a5a5f77aa0e","6c6093e634ee4d059a55ca803e15ebaa","5ad4c1e738ca48ac852dbbf45872d8c7","3c84050b4402474ba98fd6b3448f6cc9","193ba3ef87494127910e78d4f261f1e5","7bbfe97f65904e95be27c127bf13bb51","006d5ef5b32c45fe80bc9f0ed042c02e","b2a1402f75a2449dab9b45b2827d8b24","7b62c0f43d9d4a2bbf14b5ec219bd587","3f5ff187da404e0093721faa8c218d5e"]},"executionInfo":{"elapsed":16744,"status":"ok","timestamp":1651511297646,"user":{"displayName":"Eigen Ott","userId":"15016988378050056333"},"user_tz":240},"id":"TA68md-2ouhn","outputId":"52ffafeb-f77f-4391-fb37-165d678a6d3d"},"outputs":[{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/517M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7da46a6425314ae5a83531b1949154b7"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at vinai/bertweet-base were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight']\n","- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"output_type":"execute_result","data":{"text/plain":["Embedding(64001, 768, padding_idx=1)"]},"metadata":{},"execution_count":41}],"source":["from transformers import AutoModel, BertModel, AdamW, BertConfig\n","#load from pretrained\n","#bert_pretrained = BertModel.from_pretrained('bert-base-uncased')\n","bert_pretrained = AutoModel.from_pretrained(\"vinai/bertweet-base\")\n","bert_pretrained.resize_token_embeddings(len(tokenizer)) "]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11296,"status":"ok","timestamp":1651511310404,"user":{"displayName":"Eigen Ott","userId":"15016988378050056333"},"user_tz":240},"id":"W7H1TjGYYFpf","outputId":"d56667a3-cef1-4eed-e401-092d4b80f86a"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n"]},{"output_type":"stream","name":"stdout","text":["BERT_Arch(\n","  (bert): RobertaModel(\n","    (embeddings): RobertaEmbeddings(\n","      (word_embeddings): Embedding(64001, 768, padding_idx=1)\n","      (position_embeddings): Embedding(130, 768, padding_idx=1)\n","      (token_type_embeddings): Embedding(1, 768)\n","      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (encoder): RobertaEncoder(\n","      (layer): ModuleList(\n","        (0): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (1): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (2): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (3): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (4): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (5): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (6): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (7): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (8): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (9): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (10): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (11): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","    (pooler): RobertaPooler(\n","      (dense): Linear(in_features=768, out_features=768, bias=True)\n","      (activation): Tanh()\n","    )\n","  )\n","  (fc1): Linear(in_features=768, out_features=512, bias=True)\n","  (fc2): Linear(in_features=512, out_features=128, bias=True)\n","  (fc3): Linear(in_features=128, out_features=5, bias=True)\n","  (dropout1): Dropout(p=0.1, inplace=False)\n","  (dropout2): Dropout(p=0.1, inplace=False)\n","  (relu1): ReLU()\n","  (relu2): ReLU()\n","  (softmax): LogSoftmax(dim=1)\n",")\n"]}],"source":["model = BERT_Arch(bert_pretrained, output_size=len(tones))\n","\n","#print(model.embeddings.word_embeddings.weight[-1, :])\n","\n","criterion = nn.CrossEntropyLoss()\n","optimizer = AdamW(model.parameters(),\n","                  lr = 2e-5,\n","                  eps = 1e-8\n","                )\n","\n","model = model.to(device)\n","print(model)\n"]},{"cell_type":"markdown","metadata":{"id":"qUAs880MqS55"},"source":["### Accuracy Functions"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XEHbZdztqcfR"},"outputs":[],"source":["# Function to calculate the accuracy of our predictions vs labels\n","def flat_accuracy(preds, labels):\n","    pred_flat = np.argmax(preds, axis=1).flatten()\n","    labels_flat = labels.flatten()\n","    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n","\n","def multi_acc(y_pred, y_test):\n","    y_pred_softmax = torch.log_softmax(y_pred, dim = 1)\n","    _, y_pred_tags = torch.max(y_pred_softmax, dim = 1)    \n","    \n","    correct_pred = (y_pred_tags == y_test).float()\n","    acc = correct_pred.sum() / len(correct_pred)\n","    \n","    acc = torch.round(acc * 100)\n","    \n","    return acc"]},{"cell_type":"markdown","metadata":{"id":"BgPf6qY7qfgm"},"source":["## Training/validating wrapper functions"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xM7Dq7VaqezE"},"outputs":[],"source":["def training(train_dataloader, model):\n","    \n","    print('Training...')\n","\n","    t0 = time.time()\n","\n","    total_train_loss = 0\n","    total_train_accuracy = 0\n","    \n","    model.train() \n","\n","    loop = tqdm(enumerate(train_dataloader), total=len(train_dataloader))\n","    \n","    for step, batch in loop:\n","        b_input_ids = batch[0].to(device)\n","        b_input_mask = batch[1].to(device)\n","        b_labels = batch[2].to(device)\n","\n","        optimizer.zero_grad() \n","\n","        ### forward pass ###\n","        b_train_pred = model(b_input_ids, mask=b_input_mask)\n","        \n","        loss = criterion(b_train_pred, b_labels)\n","        accuracy = multi_acc(b_train_pred, b_labels)\n","        \n","        total_train_loss += loss.item() * b_input_ids.shape[0]\n","        total_train_accuracy += accuracy.item()* b_input_ids.shape[0]\n","        \n","        ### backward pass ###\n","        loss.backward()\n","#         torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n","        optimizer.step()\n","        # scheduler.step()\n","        loop.set_postfix(loss=loss.item(), acc=accuracy.item())\n","        \n","    avg_train_accuracy = total_train_accuracy / train_dataloader.dataset.__len__()\n","    avg_train_loss = total_train_loss / train_dataloader.dataset.__len__()        \n","    training_time = format_time(time.time() - t0)\n","    \n","    print(\"  Accuracy: {0:.2f}\".format(avg_train_accuracy))\n","    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n","    print(\"  Training epcoh took: {:}\".format(training_time))\n","    \n","\n","    return avg_train_accuracy, avg_train_loss, training_time"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NN8Dpj5UrccB"},"outputs":[],"source":["def validating(validation_dataloader, model):\n","    '''return validation accuracy, loss and time'''\n","\n","    print(\"Running Validation...\")\n","\n","    t0 = time.time()\n","\n","    model.eval()\n","\n","    # Tracking variables \n","    total_eval_accuracy = 0\n","    total_eval_loss = 0\n","    nb_eval_steps = 0\n","\n","    for batch in tqdm(validation_dataloader):\n","        \n","        b_input_ids = batch[0].to(device)\n","        b_input_mask = batch[1].to(device)\n","        b_labels = batch[2].to(device)\n","        \n","        with torch.no_grad():        \n","            b_train_pred = model(b_input_ids, mask=b_input_mask)\n","        \n","            loss = criterion(b_train_pred, b_labels)\n","            accuracy = multi_acc(b_train_pred, b_labels)\n","            \n","        total_eval_loss += loss.item()*b_input_ids.shape[0]\n","        total_eval_accuracy += accuracy.item()*b_input_ids.shape[0]\n","     \n","    \n","    avg_val_accuracy = total_eval_accuracy / validation_dataloader.dataset.__len__()\n","    avg_val_loss = total_eval_loss / validation_dataloader.dataset.__len__()\n","    validation_time = format_time(time.time() - t0)\n","    \n","    print(\"  Accuracy: {0:.2f}\".format(avg_val_accuracy))    \n","    print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n","    print(\"  Validation took: {:}\".format(validation_time))\n","    \n","    return avg_val_accuracy, avg_val_loss, validation_time\n"]},{"cell_type":"markdown","metadata":{"id":"wIlUJutprwOF"},"source":["# Main training procedure\n","reccomended to run on GPU if available"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ujbqaANzr1MR"},"outputs":[],"source":["seed_val = 42\n","\n","random.seed(seed_val)\n","np.random.seed(seed_val)\n","torch.manual_seed(seed_val)\n","torch.cuda.manual_seed_all(seed_val)\n","\n","training_stats = []\n","\n","epochs = 10\n","\n","total_t0 = time.time() \n","\n","for epoch_i in range(0, epochs):\n","    print(\"\")\n","    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n","    ## =================== Training =================== ##\n","    avg_train_accuracy, avg_train_loss, training_time = training(train_dataloader, model)\n","        \n","    ## =================== Validation =================== ##\n","    avg_val_accuracy, avg_val_loss, validation_time = validating(validation_dataloader, model)\n","    \n","    # Record all statistics from this epoch.\n","    training_stats.append(\n","        {\n","            'epoch': epoch_i + 1,\n","            'Training Loss': avg_train_loss,\n","            'Training Accur.': avg_train_accuracy,\n","            'Valid. Loss': avg_val_loss,\n","            'Valid. Accur.': avg_val_accuracy,\n","            'Training Time': training_time,\n","            'Validation Time': validation_time,\n","        }\n","    )\n","    \n","    #save model\n","torch.save({\n","      'epoch': epoch_i+1,\n","      'model_state_dict': model.state_dict(),\n","      'bert_state_dict': model.bert.state_dict(), #save the finetuned bert\n","      'optimizer_state_dict': optimizer.state_dict(),\n","      'avg_train_loss': avg_train_loss,\n","      }, f'./checkpoint_{epoch_i+1}.pt')\n","\n","print(\"Training complete!\")\n","\n","print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))"]},{"cell_type":"markdown","source":["# Analysis and visualizations\n","code for visualizing attention and other things\n"],"metadata":{"id":"OHXct-7RX5OM"}},{"cell_type":"code","source":["from torchviz import make_dot, make_dot_from_trace"],"metadata":{"id":"_PO_vDKiuBte"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["sentence_a = \"The cat sat on the mat\"\n","sentence_b = \"The cat lay on the rug\"\n","inputs = tokenizer.encode_plus(sentence_a, sentence_b, return_tensors='pt')\n","input_ids = inputs['input_ids'].to(device)\n","token_type_ids = inputs['token_type_ids'].to(device)\n","mask = inputs['attention_mask'].to(device)\n","\n","attention = model(input_ids, mask)"],"metadata":{"id":"_ergTYPKuELP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["param_dict = {}\n","for name, param in model.named_parameters():\n","    if param.requires_grad:\n","        param_dict[name] = param\n","    "],"metadata":{"id":"LzVZ0mRsxEho"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["make_dot(attention, params=param_dict, show_attrs=True, show_saved=True).render(\"attached\", format=\"jpeg\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":90},"id":"5Dx439KUwOfu","executionInfo":{"status":"ok","timestamp":1651512155322,"user_tz":240,"elapsed":10787,"user":{"displayName":"Eigen Ott","userId":"15016988378050056333"}},"outputId":"b6f99d4b-999a-4804-b875-e16984dd95b4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["dot: graph is too large for cairo-renderer bitmaps. Scaling by 0.735692 to fit\n","tcmalloc: large alloc 1111982080 bytes == 0x5612c55ea000 @  0x7fde83ac7001 0x7fde7fd5e1fa 0x7fde7fd5e2ad 0x7fde80ee96df 0x7fde813e0261 0x7fde8385d468 0x7fde8385fd53 0x5612c2abd092 0x7fde83212c87 0x5612c2abd12a\n"]},{"output_type":"execute_result","data":{"text/plain":["'attached.jpeg'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":68}]},{"cell_type":"code","source":[""],"metadata":{"id":"tfipK9mEwtlV"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"name":"BERT.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"75327c0805c540139344cd13775dad5f":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_54899fe4c6cc4d99be2a984eca860ada","IPY_MODEL_62569192ccaa44fcaf5be39af207d215","IPY_MODEL_f4515028018e41c38f63ce0b3eb68a94"],"layout":"IPY_MODEL_fb694c220095436c907503c6c85a2d8a"}},"54899fe4c6cc4d99be2a984eca860ada":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7232f5f528c84b16bb78d4e352817dcd","placeholder":"‚Äã","style":"IPY_MODEL_b8834817f001469f9c8935e7a057e74d","value":"Downloading: 100%"}},"62569192ccaa44fcaf5be39af207d215":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_96a46f254b554a1bb44aa3edc7cd4d9f","max":558,"min":0,"orientation":"horizontal","style":"IPY_MODEL_8931cd75a3064a3e9c945aac5c046982","value":558}},"f4515028018e41c38f63ce0b3eb68a94":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e48e2a66020a4503a2463c69abfaa832","placeholder":"‚Äã","style":"IPY_MODEL_e4e6da985d76452f8592ac2777fddca3","value":" 558/558 [00:00&lt;00:00, 13.5kB/s]"}},"fb694c220095436c907503c6c85a2d8a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7232f5f528c84b16bb78d4e352817dcd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b8834817f001469f9c8935e7a057e74d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"96a46f254b554a1bb44aa3edc7cd4d9f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8931cd75a3064a3e9c945aac5c046982":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"e48e2a66020a4503a2463c69abfaa832":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e4e6da985d76452f8592ac2777fddca3":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"35d8c1fe62264e288919a608b663c2a2":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_4548174d0a454bddbd66c67db2f29782","IPY_MODEL_63e8fe9e66eb4cf4a7464b1e775500a2","IPY_MODEL_fedb4684404f4d53b1114455c9f37841"],"layout":"IPY_MODEL_a3269b135b4e448487d41b0f71214d8e"}},"4548174d0a454bddbd66c67db2f29782":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5dd00853ffb04eaea21a7f69df5fcf4a","placeholder":"‚Äã","style":"IPY_MODEL_b07421e0e7e643d6a1669b70d4661192","value":"Downloading: 100%"}},"63e8fe9e66eb4cf4a7464b1e775500a2":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_530ba5b67f9b4fdd890872b99aacacbe","max":843438,"min":0,"orientation":"horizontal","style":"IPY_MODEL_cbc4d1238878448ea9406625863c7f07","value":843438}},"fedb4684404f4d53b1114455c9f37841":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_fb65c6f50d584287ac6bf8905b2feeb6","placeholder":"‚Äã","style":"IPY_MODEL_7ffdfff774744073b967a7a7afbdcfbc","value":" 824k/824k [00:00&lt;00:00, 808kB/s]"}},"a3269b135b4e448487d41b0f71214d8e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5dd00853ffb04eaea21a7f69df5fcf4a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b07421e0e7e643d6a1669b70d4661192":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"530ba5b67f9b4fdd890872b99aacacbe":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cbc4d1238878448ea9406625863c7f07":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"fb65c6f50d584287ac6bf8905b2feeb6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7ffdfff774744073b967a7a7afbdcfbc":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ff5cca2f47f54621a1523329e416c350":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_30f9606a5247433e8cf29b439aaaf31b","IPY_MODEL_3c14376fe3ad460c9acd9ade6f75219e","IPY_MODEL_b7530abbae114d00ba6da5f44f5430c8"],"layout":"IPY_MODEL_7fe41d5539fc4e2d8de551de57734396"}},"30f9606a5247433e8cf29b439aaaf31b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e04a06f09e6e47f38800e9a2d1bff60b","placeholder":"‚Äã","style":"IPY_MODEL_a66d8485ce4e4bf6b01260fc1d7bf9e7","value":"Downloading: 100%"}},"3c14376fe3ad460c9acd9ade6f75219e":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_75dc32e3021f4f7e892b35ef613dcad0","max":1078931,"min":0,"orientation":"horizontal","style":"IPY_MODEL_1ed5352d7d124896b041bf00a6cb6604","value":1078931}},"b7530abbae114d00ba6da5f44f5430c8":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f280f4bb66d141f09dbf39c01fb3f96c","placeholder":"‚Äã","style":"IPY_MODEL_5d0f7b32128749c7b4dee243be0400ff","value":" 1.03M/1.03M [00:00&lt;00:00, 2.10MB/s]"}},"7fe41d5539fc4e2d8de551de57734396":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e04a06f09e6e47f38800e9a2d1bff60b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a66d8485ce4e4bf6b01260fc1d7bf9e7":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"75dc32e3021f4f7e892b35ef613dcad0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1ed5352d7d124896b041bf00a6cb6604":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"f280f4bb66d141f09dbf39c01fb3f96c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5d0f7b32128749c7b4dee243be0400ff":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7da46a6425314ae5a83531b1949154b7":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_35a7a8866ac94c4a80c69a5a5f77aa0e","IPY_MODEL_6c6093e634ee4d059a55ca803e15ebaa","IPY_MODEL_5ad4c1e738ca48ac852dbbf45872d8c7"],"layout":"IPY_MODEL_3c84050b4402474ba98fd6b3448f6cc9"}},"35a7a8866ac94c4a80c69a5a5f77aa0e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_193ba3ef87494127910e78d4f261f1e5","placeholder":"‚Äã","style":"IPY_MODEL_7bbfe97f65904e95be27c127bf13bb51","value":"Downloading: 100%"}},"6c6093e634ee4d059a55ca803e15ebaa":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_006d5ef5b32c45fe80bc9f0ed042c02e","max":542529064,"min":0,"orientation":"horizontal","style":"IPY_MODEL_b2a1402f75a2449dab9b45b2827d8b24","value":542529064}},"5ad4c1e738ca48ac852dbbf45872d8c7":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7b62c0f43d9d4a2bbf14b5ec219bd587","placeholder":"‚Äã","style":"IPY_MODEL_3f5ff187da404e0093721faa8c218d5e","value":" 517M/517M [00:14&lt;00:00, 41.2MB/s]"}},"3c84050b4402474ba98fd6b3448f6cc9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"193ba3ef87494127910e78d4f261f1e5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7bbfe97f65904e95be27c127bf13bb51":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"006d5ef5b32c45fe80bc9f0ed042c02e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b2a1402f75a2449dab9b45b2827d8b24":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"7b62c0f43d9d4a2bbf14b5ec219bd587":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3f5ff187da404e0093721faa8c218d5e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}