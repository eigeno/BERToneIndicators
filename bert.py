# -*- coding: utf-8 -*-
"""BERT.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1LuWGCt5IUadeHOX8HRxwF0MgidB5bc0t

# TODO
- Data Analysis
  <!-- - distribution -->
  - visualization
- Preprocessing
  - cleaning the labels
  <!-- - Tokenization -->
  - Data Load, class balancing?, k-cross fold validation?
- Model
  <!-- - BERT from pretrained -->
  <!-- - optizer, scheduler, criteria etc. -->
<!-- - Training Procedure -->
- Testing
"""

import os

import numpy as np
print('numpy', np.__version__)
import pandas as pd
print('pandas', pd.__version__)
import random
import re

import seaborn as sns
import matplotlib.pyplot as plt

import torch
import torch.nn as nn
import torch.nn.functional as F
#from torchtext.data import Field, TabularDataset, BucketIterator, Iterator

from transformers import AutoTokenizer, BertTokenizer, BertForSequenceClassification

from tqdm import tqdm #progress bar

# Evaluation

from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

import time
import datetime

def format_time(elapsed):
    '''
    Takes a time in seconds and returns a string hh:mm:ss
    '''
    # Round to the nearest second.
    elapsed_rounded = int(round((elapsed)))

    # Format as hh:mm:ss
    return str(datetime.timedelta(seconds=elapsed_rounded))

from torch import cuda

device = 'cuda' if cuda.is_available() else 'cpu'
print('Device count:', cuda.device_count())
print('Using device:', device)

print()

#Additional Info when using cuda
if device == 'cuda':
    print(torch.cuda.get_device_name(0))
    print(torch.cuda.get_device_properties(0))
    print('Memory Usage:')
    print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')
    print('Cached:   ', round(torch.cuda.memory_cached(0)/1024**3,1), 'GB')



"""# Data Analysis"""

data = pd.read_csv('all_tweets.tsv', delimiter='\t', index_col=0, on_bad_lines='skip')
print(data.columns)
data

# sum_data = data['srs']+data['nbh']+data['hyp']+data['pos']+data['neg']
# count = 0
# for i, c in enumerate(sum_data):
#   if c >1:
#     print(i, c, data['text'][i])
#     count +=1
# print(count)

#rate of more than 1 label
print('The rate of tweets with more than 1 tags:', 111/10776)

"""# Data Preprocessing

"""

contain_values = data[data['text'].str.contains('/srs|/nbh|/hyp|/pos|/neg')]
pd.options.mode.chained_assignment = None  # default='warn'
labeled = contain_values
labeled["srs"] = np.where(labeled["text"].str.contains('/srs'), 1, 0)
labeled["nbh"] = np.where(labeled["text"].str.contains('/nbh'), 1, 0)
labeled["hyp"] = np.where(labeled["text"].str.contains('/hyp'), 1, 0)
labeled["pos"] = np.where(labeled["text"].str.contains('/pos'), 1, 0)
labeled["neg"] = np.where(labeled["text"].str.contains('/neg'), 1, 0)
data = labeled

# TODO
# Filter for english only tweets
# filter out tweets that are too short (how short is too short?)

"""## Tokenization"""

#remove the tweets with more than 1 tag
tones = ['srs', 'nbh', 'hyp', 'pos', 'neg']

filter = data[tones].sum(axis=1)==1
data = data[filter].reset_index(drop=True)
print(data.info())
print(data[tones].sum(axis=0))

data['tag']=data[tones].apply(lambda row: row[row==1].index[0], axis=1)

label2ind = {tag: tones.index(tag) for tag in tones}
tags_token= data['tag'].apply(lambda x: label2ind[x])

print(data)

#TODO replace all the tags in texts with [unknown token]
data['text'] = data['text'].str.replace('/srs','TONE_INDICATOR')
data['text'] = data['text'].str.replace('/nbh','TONE_INDICATOR')
data['text'] = data['text'].str.replace('/pos','TONE_INDICATOR')
data['text'] = data['text'].str.replace('/hyp','TONE_INDICATOR')
data['text'] = data['text'].str.replace('/neg','TONE_INDICATOR')

#tokenize sentences
tokenizer = AutoTokenizer.from_pretrained("vinai/bertweet-base", use_fast=False)
print("length before new token", len(tokenizer))
#tokenizer.add_tokens(["TONE_INDICATOR"])
print("length after new token", len(tokenizer))

tweets = data['text']
print(tweets[23])
print(' Original: ', tweets[23])
print('Tokenized: ', tokenizer.tokenize(tweets[23]))
print('Token IDs: ', tokenizer.convert_tokens_to_ids(tokenizer.tokenize(tweets[23])))

#get max length of sentence token
max_len = 0
count = 0

for twt in tweets:
    input_ids = tokenizer.encode(twt, add_special_tokens=True) #add `[CLS]` and `[SEP]` tokens.
    max_len = max(max_len, len(input_ids))

print('Max sentence length: ', max_len)

# tokenizer.max_len_sentences_pair, tokenizer.max_len_single_sentence

input_ids = []
attention_masks = []

for twt in tweets:
    encoded_dict = tokenizer.encode_plus(
                        twt,                      # Sentence to encode.
                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'
                        max_length = 124,           # Pad & truncate all sentences.
                        pad_to_max_length = True,
                        return_attention_mask = True,   # Construct attn. masks.
                        return_tensors = 'pt',     # Return pytorch tensors.
                   )

    input_ids.append(encoded_dict['input_ids'])
    attention_masks.append(encoded_dict['attention_mask'])


input_ids = torch.cat(input_ids, dim=0)
attention_masks = torch.cat(attention_masks, dim=0)
labels = torch.tensor(tags_token)

# Print sentence 0, now as a list of IDs.
print('Original: ', tweets[0])
print('Token IDs:', input_ids[0])

"""# Train/test split, dataloader"""

from torch.utils.data import TensorDataset, random_split

dataset = TensorDataset(input_ids, attention_masks, labels)

# Create a 90-10 train-validation split.
train_size = int(0.9 * len(dataset))
val_size = len(dataset) - train_size
train_dataset, val_dataset = random_split(dataset, [train_size, val_size])

print('{:>5,} training samples'.format(train_size))
print('{:>5,} validation samples'.format(val_size))

from torch.utils.data import DataLoader, RandomSampler, SequentialSampler

# The DataLoader needs to know our batch size for training, so we specify it
# here. For fine-tuning BERT on a specific task, the authors recommend a batch
# size of 16 or 32.
batch_size = 32

# Create the DataLoaders for our training and validation sets.
# We'll take training samples in random order.
train_dataloader = DataLoader(
            train_dataset,  # The training samples.
            sampler = RandomSampler(train_dataset), # Select batches randomly
            batch_size = batch_size # Trains with this batch size.
        )

# For validation the order doesn't matter, so we'll just read them sequentially.
validation_dataloader = DataLoader(
            val_dataset, # The validation samples.
            sampler = SequentialSampler(val_dataset), # Pull out batches sequentially.
            batch_size = batch_size # Evaluate with this batch size.
        )

"""# Model
model architecture:
pretrained-BERT + classifier layer
"""

class BERT_Arch(nn.Module):
    def __init__(self, bert, output_size):
        super(BERT_Arch, self).__init__()

        self.bert = bert

        self.fc1 = nn.Linear(768,512)
        self.fc2 = nn.Linear(512, 128)
        self.fc3 = nn.Linear(128, output_size)

        #activation functions
        self.dropout1 = nn.Dropout(0.1)
        self.dropout2 = nn.Dropout(0.1)
        self.relu1 =  nn.ReLU()
        self.relu2 =  nn.ReLU()
        self.softmax = nn.LogSoftmax(dim=1)

    def forward(self, sent_id, mask):

        bert_out = self.bert(sent_id, attention_mask=mask).pooler_output #(batch_size, hidden_size)

        x = self.fc1(bert_out)
        x = self.relu1(x)
        x = self.dropout1(x)

        x = self.fc2(x)
        x = self.relu2(x)
        x = self.dropout2(x)

        x = self.fc3(x)
        x = self.softmax(x)

        return x



from transformers import AutoModel, BertModel, AdamW, BertConfig
#load from pretrained
#bert_pretrained = BertModel.from_pretrained('bert-base-uncased')
bert_pretrained = AutoModel.from_pretrained("vinai/bertweet-base")
bert_pretrained.resize_token_embeddings(len(tokenizer))

model = BERT_Arch(bert_pretrained, output_size=len(tones))

#print(model.embeddings.word_embeddings.weight[-1, :])

criterion = nn.CrossEntropyLoss()
optimizer = AdamW(model.parameters(),
                  lr = 2e-5,
                  eps = 1e-8
                )

model = model.to(device)
print(model)

"""### Accuracy Functions"""

# Function to calculate the accuracy of our predictions vs labels
def flat_accuracy(preds, labels):
    pred_flat = np.argmax(preds, axis=1).flatten()
    labels_flat = labels.flatten()
    return np.sum(pred_flat == labels_flat) / len(labels_flat)

def multi_acc(y_pred, y_test):
    y_pred_softmax = torch.log_softmax(y_pred, dim = 1)
    _, y_pred_tags = torch.max(y_pred_softmax, dim = 1)

    correct_pred = (y_pred_tags == y_test).float()
    acc = correct_pred.sum() / len(correct_pred)

    acc = torch.round(acc * 100)

    return acc

"""## Training/validating wrapper functions"""

def training(train_dataloader, model):

    print('Training...')

    t0 = time.time()

    total_train_loss = 0
    total_train_accuracy = 0

    model.train()

    loop = tqdm(enumerate(train_dataloader), total=len(train_dataloader))

    for step, batch in loop:
        b_input_ids = batch[0].to(device)
        b_input_mask = batch[1].to(device)
        b_labels = batch[2].to(device)

        optimizer.zero_grad()

        ### forward pass ###
        b_train_pred = model(b_input_ids, mask=b_input_mask)

        loss = criterion(b_train_pred, b_labels)
        accuracy = multi_acc(b_train_pred, b_labels)

        total_train_loss += loss.item() * b_input_ids.shape[0]
        total_train_accuracy += accuracy.item()* b_input_ids.shape[0]

        ### backward pass ###
        loss.backward()
#         torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
        optimizer.step()
        # scheduler.step()
        loop.set_postfix(loss=loss.item(), acc=accuracy.item())

    avg_train_accuracy = total_train_accuracy / train_dataloader.dataset.__len__()
    avg_train_loss = total_train_loss / train_dataloader.dataset.__len__()
    training_time = format_time(time.time() - t0)

    print("  Accuracy: {0:.2f}".format(avg_train_accuracy))
    print("  Average training loss: {0:.2f}".format(avg_train_loss))
    print("  Training epcoh took: {:}".format(training_time))


    return avg_train_accuracy, avg_train_loss, training_time

def validating(validation_dataloader, model):
    '''return validation accuracy, loss and time'''

    print("Running Validation...")

    t0 = time.time()

    model.eval()

    # Tracking variables
    total_eval_accuracy = 0
    total_eval_loss = 0
    nb_eval_steps = 0

    for batch in tqdm(validation_dataloader):

        b_input_ids = batch[0].to(device)
        b_input_mask = batch[1].to(device)
        b_labels = batch[2].to(device)

        with torch.no_grad():
            b_train_pred = model(b_input_ids, mask=b_input_mask)

            loss = criterion(b_train_pred, b_labels)
            accuracy = multi_acc(b_train_pred, b_labels)

        total_eval_loss += loss.item()*b_input_ids.shape[0]
        total_eval_accuracy += accuracy.item()*b_input_ids.shape[0]


    avg_val_accuracy = total_eval_accuracy / validation_dataloader.dataset.__len__()
    avg_val_loss = total_eval_loss / validation_dataloader.dataset.__len__()
    validation_time = format_time(time.time() - t0)

    print("  Accuracy: {0:.2f}".format(avg_val_accuracy))
    print("  Validation Loss: {0:.2f}".format(avg_val_loss))
    print("  Validation took: {:}".format(validation_time))

    return avg_val_accuracy, avg_val_loss, validation_time

"""# Main training procedure"""

seed_val = 42

random.seed(seed_val)
np.random.seed(seed_val)
torch.manual_seed(seed_val)
torch.cuda.manual_seed_all(seed_val)

training_stats = []

epochs = 10

total_t0 = time.time()

for epoch_i in range(0, epochs):
    print("")
    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))
    ## =================== Training =================== ##
    avg_train_accuracy, avg_train_loss, training_time = training(train_dataloader, model)

    ## =================== Validation =================== ##
    avg_val_accuracy, avg_val_loss, validation_time = validating(validation_dataloader, model)

    # Record all statistics from this epoch.
    training_stats.append(
        {
            'epoch': epoch_i + 1,
            'Training Loss': avg_train_loss,
            'Training Accur.': avg_train_accuracy,
            'Valid. Loss': avg_val_loss,
            'Valid. Accur.': avg_val_accuracy,
            'Training Time': training_time,
            'Validation Time': validation_time,
        }
    )

    #save model
torch.save({
      'epoch': epoch_i+1,
      'model_state_dict': model.state_dict(),
      'optimizer_state_dict': optimizer.state_dict(),
      'avg_train_loss': avg_train_loss,
      }, f'./checkpoint_{epoch_i+1}.pt')

print("Training complete!")

print("Total training took {:} (h:mm:ss)".format(format_time(time.time()-total_t0)))
